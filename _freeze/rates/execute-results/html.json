{
  "hash": "7bdbd12b42c4adee62af3032f65172c5",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: Theoretical QM Rates vs. Experimental Heuristic Rates\n---\n\n\n\nKIMMDY can be used to sample competing reactions as long as proxies for comparable reaction rates are known.\nLet us consider the following two reactions that can occur in the backbone of a protein:\n\n1. Homolysis of a bond\n2. Hydrolysis of a bond\n\nThe KIMMDY-plugin version of earlier work on collagen bond rupture by @rennekampHybridKineticMonte2020 shipped with KIMMDY in the optional [kimmdy-reactons](https://graeter-group.github.io/kimmdy-reactions/) package can be used to simulate force-dependent bond rupture reactions.\n\n$$\nk=A e^{-\\Delta E / k_B T}\n$${#eq-rate}\n\n@rennekampHybridKineticMonte2020 ultimately used an empirical attempt frequency of $A=0.23 ps^{-1}$ (after starting with the theoretical maximal attempt frequency of $A=6.25 ps^{-1}$).\n\nFor hydrolysis, we take a a baseline energy barrier scaled by the force-dependence of the reaction unveiled by @pillMechanicalActivationDrastically2019.\nThe observed attempt frequency of the hydrolysis reaction is further scaled by the solvent accessible surface area (SASA) of the peptide bond, normalized\nto the maximum SASA of a peptide bond in a lonely glycine-glycine dipeptide ($=140 A^2$).\n\n@pillMechanicalActivationDrastically2019 suggest that the rate of the hydrolysis reaction is govered by two enegy barriers, where T1 is\nalmost insensitive to mechanical activation, but only becomes rate-determining after the mechanically sensitive T2 is lowered by force.\n\nTS1:\n\n- 80 kJ/mol at 0nN\n- 77 kJ/mol at 1.8nN\n- E = 80 - 1.67 * F\n\nTS2:\n\n- 92.5 kJ/mol at 0nN\n- 46 kJ/mol at 1.8 nN\n- E = 92.5 - 25.83 * F\n\nThis translates to:\n\n```python\ncritical_force = 0.7 # nN\nif force < critical_force\n  # low force regime, TS2 is rate-determining\n  E_barrier = 92.5 - 25.83 * force\nelse:\n  # high force regime, TS1 is rate-determining\n  E_barrier = 80 - 1.67 * force\n```\n\nBut the theortical rates don't quite match the experimental rates.\n\n\n::: {#63de92f7 .cell execution_count=1}\n``` {.python .cell-code}\nfrom plotnine import * # pyright: ignore\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nfrom kimmdy_hydrolysis.rates import high_force_log_rate, theoretical_reaction_rate_per_s, experimental_reaction_rate_per_s, low_force_log_rate\nimport pandas as pd\nimport os\n\nroot = os.getcwd()\n\ntheme_set(theme_minimal())\n```\n\n::: {.cell-output .cell-output-display execution_count=31}\n```\n<plotnine.themes.theme_minimal.theme_minimal at 0x7782a2d13310>\n```\n:::\n:::\n\n\n\n\nInstead of calculating rates from the theoretical energies we can fit the rates to the experimental data directly.\n\nGet the data from the SI and add predicted rates from hydrolysis plugin function:\n\n::: {#22228d16 .cell execution_count=3}\n``` {.python .cell-code}\ndf = pd.read_csv(\"./assets/afm-rates.csv\")\ndf = df.melt(id_vars=[\"f\"], var_name=\"t\", value_name=\"rate\")\ndf[\"t\"] = pd.to_numeric(df[\"t\"], errors=\"coerce\")\ndf = df.dropna(subset=[\"rate\"])\ndf[\"log_k\"] = np.log(df[\"rate\"])\ndf[\"is_low_f\"] = df[\"f\"] <= 0.7\ndf['plugin_k'] = [experimental_reaction_rate_per_s(force=f, temperature=t) for f, t in zip(df[\"f\"], df[\"t\"])]\ndf['plugin_theo_k'] = [theoretical_reaction_rate_per_s(force=f, temperature=t) for f, t in zip(df[\"f\"], df[\"t\"])]\ndf['plugin_high_log_k'] = [high_force_log_rate(force=f, temperature=t) for f, t in zip(df[\"f\"], df[\"t\"])]\ndf['plugin_low_log_k'] = [low_force_log_rate(force=f) for f, t in zip(df[\"f\"], df[\"t\"])]\ndf['plugin_log_k'] = np.log(df['plugin_k'])\n```\n:::\n\n\nFit model log(k) ~ t + f:\n\n::: {#9e483349 .cell execution_count=4}\n``` {.python .cell-code}\nimport statsmodels.api as sm\n\nmodel = sm.OLS.from_formula(\"log_k ~ t + f\", data=df.query(\"not is_low_f\"))\nmodel.fit().summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=33}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>log_k</td>      <th>  R-squared:         </th> <td>   0.944</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.939</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   187.0</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 12 Jun 2025</td> <th>  Prob (F-statistic):</th> <td>1.56e-14</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>14:32:26</td>     <th>  Log-Likelihood:    </th> <td>  14.858</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>    25</td>      <th>  AIC:               </th> <td>  -23.72</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>    22</td>      <th>  BIC:               </th> <td>  -20.06</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>  -20.3430</td> <td>    1.946</td> <td>  -10.453</td> <td> 0.000</td> <td>  -24.379</td> <td>  -16.307</td>\n</tr>\n<tr>\n  <th>t</th>         <td>    0.0706</td> <td>    0.006</td> <td>   10.940</td> <td> 0.000</td> <td>    0.057</td> <td>    0.084</td>\n</tr>\n<tr>\n  <th>f</th>         <td>    1.6052</td> <td>    0.101</td> <td>   15.946</td> <td> 0.000</td> <td>    1.396</td> <td>    1.814</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td> 3.044</td> <th>  Durbin-Watson:     </th> <td>   2.069</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td> 0.218</td> <th>  Jarque-Bera (JB):  </th> <td>   2.094</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.523</td> <th>  Prob(JB):          </th> <td>   0.351</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 2.044</td> <th>  Cond. No.          </th> <td>2.06e+04</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.<br/>[2] The condition number is large, 2.06e+04. This might indicate that there are<br/>strong multicollinearity or other numerical problems.\n```\n:::\n:::\n\n\n::: {#d46d393a .cell execution_count=5}\n``` {.python .cell-code}\nmodel_294k = sm.OLS.from_formula(\"log_k ~ f\", data=df.query(\"not is_low_f & t == 294.15\"))\nmodel_294k.fit().summary()\n```\n\n::: {.cell-output .cell-output-display execution_count=34}\n```{=html}\n<table class=\"simpletable\">\n<caption>OLS Regression Results</caption>\n<tr>\n  <th>Dep. Variable:</th>          <td>log_k</td>      <th>  R-squared:         </th> <td>   0.981</td>\n</tr>\n<tr>\n  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.975</td>\n</tr>\n<tr>\n  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   159.0</td>\n</tr>\n<tr>\n  <th>Date:</th>             <td>Thu, 12 Jun 2025</td> <th>  Prob (F-statistic):</th>  <td>0.00108</td>\n</tr>\n<tr>\n  <th>Time:</th>                 <td>14:32:26</td>     <th>  Log-Likelihood:    </th> <td>  6.8454</td>\n</tr>\n<tr>\n  <th>No. Observations:</th>      <td>     5</td>      <th>  AIC:               </th> <td>  -9.691</td>\n</tr>\n<tr>\n  <th>Df Residuals:</th>          <td>     3</td>      <th>  BIC:               </th> <td>  -10.47</td>\n</tr>\n<tr>\n  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>   \n</tr>\n<tr>\n  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n</tr>\n<tr>\n  <th>Intercept</th> <td>    0.4216</td> <td>    0.155</td> <td>    2.722</td> <td> 0.072</td> <td>   -0.071</td> <td>    0.914</td>\n</tr>\n<tr>\n  <th>f</th>         <td>    1.5839</td> <td>    0.126</td> <td>   12.608</td> <td> 0.001</td> <td>    1.184</td> <td>    1.984</td>\n</tr>\n</table>\n<table class=\"simpletable\">\n<tr>\n  <th>Omnibus:</th>       <td>   nan</td> <th>  Durbin-Watson:     </th> <td>   2.105</td>\n</tr>\n<tr>\n  <th>Prob(Omnibus):</th> <td>   nan</td> <th>  Jarque-Bera (JB):  </th> <td>   0.617</td>\n</tr>\n<tr>\n  <th>Skew:</th>          <td> 0.204</td> <th>  Prob(JB):          </th> <td>   0.734</td>\n</tr>\n<tr>\n  <th>Kurtosis:</th>      <td> 1.328</td> <th>  Cond. No.          </th> <td>    8.80</td>\n</tr>\n</table><br/><br/>Notes:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n```\n:::\n:::\n\n\nadd model predictions:\n\n::: {#eebd605f .cell execution_count=6}\n``` {.python .cell-code}\ndf['model_high_log_k'] = model.fit().predict(df.query(\"not is_low_f\"))\ndf['model_294k_log_k'] = model_294k.fit().predict(df.query(\"not is_low_f & t == 294.15\"))\n```\n:::\n\n\nArrhenius plot:\n\n::: {#2cb894a3 .cell execution_count=7}\n``` {.python .cell-code}\np = (\n  ggplot(df.query('not is_low_f'), aes(x='t', y='log_k', color='factor(f)')) +\n  geom_smooth(method='lm', se=False) +\n  geom_point(size=5) +\n  geom_point(aes(y='plugin_high_log_k'), size=5, shape='x') +\n  geom_point(aes(y='model_high_log_k'), size=5, shape='.') +\n  geom_point(aes(y='model_294k_log_k'), size=5, shape='.', color='red') +\n  scale_x_reverse() +\n  guides(color=guide_legend(reverse=True)) +\n  labs(\n    y='log(k) [1/s]',\n    x='Temperature [K]',\n    color='Force [nN]'\n  )\n)\np.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](rates_files/figure-html/cell-8-output-1.png){width=672 height=480}\n:::\n:::\n\n\n::: {#52ee82ab .cell execution_count=8}\n``` {.python .cell-code}\np = (\n  ggplot(df.query('not is_low_f & t == 294.15'), aes(x='f', y='log_k')) +\n  geom_smooth(method='lm', se=False) +\n  geom_point(size=5) +\n  geom_point(aes(y='plugin_high_log_k'), size=5, shape='x') +\n  geom_point(aes(y='model_high_log_k'), size=5, shape='.') +\n  geom_point(aes(y='model_294k_log_k'), size=5, shape='.', color='red') +\n  scale_x_reverse() +\n  guides(color=guide_legend(reverse=True)) +\n  labs(\n    x='Force [nN]',\n    y='log(k) [1/s]',\n  )\n)\np.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](rates_files/figure-html/cell-9-output-1.png){width=672 height=480}\n:::\n:::\n\n\n::: {#d5cb5d67 .cell execution_count=9}\n``` {.python .cell-code}\np = (\n  ggplot(df, aes(x='f', y='rate', color='factor(t)')) +\n  # geom_smooth(method='lm', se=False) +\n  geom_point(size=5, color='blue') +\n  # geom_point(aes(y='plugin_log_k'), size=5, shape='x') +\n  geom_point(aes(y='plugin_theo_k'), size=5, shape='.', color='black') +\n  scale_x_reverse() +\n  scale_y_log10() +\n  guides(color=guide_legend(reverse=True)) +\n  labs(\n    y='log(k) [1/s]',\n    x='Force [nN]',\n    color='Temperature [K]'\n  )\n)\np.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](rates_files/figure-html/cell-10-output-1.png){width=672 height=480}\n:::\n:::\n\n\n",
    "supporting": [
      "rates_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\" data-relocate-top=\"true\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}